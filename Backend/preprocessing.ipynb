{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"2fdf56fc1aa346bd902c332ccf4166be":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3e90e01f2406459c9ef8da084b397fe6","IPY_MODEL_a537cabfd10647e785b124b9d1c3fa53","IPY_MODEL_136612d43211445b86df53d153d77cf2"],"layout":"IPY_MODEL_41fe1cba59ef446cac056e9240a0b6b3"}},"3e90e01f2406459c9ef8da084b397fe6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_af52a9b8477e40a4933abf1b7f6878aa","placeholder":"​","style":"IPY_MODEL_e9e5f01a478d40f78d6f77e81a6dd97f","value":"100%"}},"a537cabfd10647e785b124b9d1c3fa53":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_980678cc21a64ce4baf385df8089ab0d","max":47,"min":0,"orientation":"horizontal","style":"IPY_MODEL_14f2904e6ece4edc88b6137c6a23873c","value":47}},"136612d43211445b86df53d153d77cf2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_62f1097a2a7d4f10a0593105d6aee620","placeholder":"​","style":"IPY_MODEL_c4c053125af8476192e70f45169caa39","value":" 47/47 [08:43&lt;00:00, 11.06s/it]"}},"41fe1cba59ef446cac056e9240a0b6b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af52a9b8477e40a4933abf1b7f6878aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9e5f01a478d40f78d6f77e81a6dd97f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"980678cc21a64ce4baf385df8089ab0d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"14f2904e6ece4edc88b6137c6a23873c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"62f1097a2a7d4f10a0593105d6aee620":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4c053125af8476192e70f45169caa39":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"nMI7CYxcyiUB"},"source":["Before running the file Upload all your data set on your goole drive in a zip format"]},{"cell_type":"code","metadata":{"id":"YjtnZQkTu6tX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703671681527,"user_tz":-330,"elapsed":17573,"user":{"displayName":"Jahan Sai","userId":"17887681191178947053"}},"outputId":"1e9a5428-074e-402b-e36f-d5ebdce45098"},"source":["#Mount our google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"f4y_fGlmur4v","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703671475139,"user_tz":-330,"elapsed":630,"user":{"displayName":"Jahan Sai","userId":"17887681191178947053"}},"outputId":"9e692184-4309-4e91-b2ba-013feeb71837"},"source":["#before running this please change the RUNTIME to GPU (Runtime -> Change runtime type -> set harware accelarotor as GPU)\n","#download and unzip the data from google drive Colab environment\n","from google_drive_downloader import GoogleDriveDownloader as gdd\n","#use only file id of the link\n","#Note: Below link is just an example, Not an actual link. Actual Links are in ReadMe file\n","#https://drive.google.com/file/d/1ubvKLzBDe5i1acxgGUK6ObeNBYCKUS07/view?usp=sharing\n","url = '1ubvKLzBDe5i1acxgGUK6ObeNBYCKUS07'\n","gdd.download_file_from_google_drive(file_id = url,dest_path='./data.zip',unzip=True)"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading 1ubvKLzBDe5i1acxgGUK6ObeNBYCKUS07 into ./data.zip... Done.\n","Unzipping..."]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/google_drive_downloader/google_drive_downloader.py:78: UserWarning: Ignoring `unzip` since \"1ubvKLzBDe5i1acxgGUK6ObeNBYCKUS07\" does not look like a valid zip file\n","  warnings.warn('Ignoring `unzip` since \"{}\" does not look like a valid zip file'.format(file_id))\n"]}]},{"cell_type":"code","metadata":{"id":"1f40EeRuvAkO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703671716160,"user_tz":-330,"elapsed":25932,"user":{"displayName":"Jahan Sai","userId":"17887681191178947053"}},"outputId":"eb8a13b1-eb0c-4528-8883-764390751930"},"source":["#To get the average frame count\n","import json\n","import glob\n","import numpy as np\n","import cv2\n","import copy\n","#change the path accordingly\n","video_files =  glob.glob('/content/drive/MyDrive/deepfake_real/*.mp4')\n","#video_files1 =  glob.glob('/content/dfdc_train_part_0/*.mp4')\n","#video_files += video_files1\n","frame_count = []\n","for video_file in video_files:\n","  cap = cv2.VideoCapture(video_file)\n","  if(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))<150):\n","    video_files.remove(video_file)\n","    continue\n","  frame_count.append(int(cap.get(cv2.CAP_PROP_FRAME_COUNT)))\n","print(\"frames\" , frame_count)\n","print(\"Total number of videos: \" , len(frame_count))\n","print('Average frame per video:',np.mean(frame_count))"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["frames [437, 215, 566, 378, 514, 448, 364, 387, 570, 492, 420, 456, 314, 461, 401, 362, 437, 448, 314, 362, 456, 437, 461, 420, 448, 401, 364, 362, 314, 401, 437, 420, 314, 401, 448, 364, 448, 448, 362, 456, 314, 401, 461, 364, 437, 362, 420]\n","Total number of videos:  47\n","Average frame per video: 409.93617021276594\n"]}]},{"cell_type":"code","metadata":{"id":"U92Ovn3JvV52","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703671757372,"user_tz":-330,"elapsed":38378,"user":{"displayName":"Jahan Sai","userId":"17887681191178947053"}},"outputId":"33de8565-fe94-4d4b-ff69-8dcc6fce3132"},"source":["# to extract frame\n","def frame_extract(path):\n","  vidObj = cv2.VideoCapture(path)\n","  success = 1\n","  while success:\n","      success, image = vidObj.read()\n","      if success:\n","          yield image\n","!pip3 install face_recognition\n","!mkdir '/content/drive/My Drive/Face_data'\n","import torch\n","import torchvision\n","from torchvision import transforms\n","from torch.utils.data import DataLoader\n","from torch.utils.data.dataset import Dataset\n","import os\n","import numpy as np\n","import cv2\n","import matplotlib.pyplot as plt\n","import face_recognition\n","from tqdm.autonotebook import tqdm\n","# process the frames\n","def create_face_videos(path_list,out_dir):\n","  already_present_count =  glob.glob(out_dir+'*.mp4')\n","  print(\"No of videos already present \" , len(already_present_count))\n","  for path in tqdm(path_list):\n","    out_path = os.path.join(out_dir,path.split('/')[-1])\n","    file_exists = glob.glob(out_path)\n","    if(len(file_exists) != 0):\n","      print(\"File Already exists: \" , out_path)\n","      continue\n","    frames = []\n","    flag = 0\n","    face_all = []\n","    frames1 = []\n","    out = cv2.VideoWriter(out_path,cv2.VideoWriter_fourcc('M','J','P','G'), 30, (112,112))\n","    for idx,frame in enumerate(frame_extract(path)):\n","      #if(idx % 3 == 0):\n","      if(idx <= 150):\n","        frames.append(frame)\n","        if(len(frames) == 4):\n","          faces = face_recognition.batch_face_locations(frames)\n","          for i,face in enumerate(faces):\n","            if(len(face) != 0):\n","              top,right,bottom,left = face[0]\n","            try:\n","              out.write(cv2.resize(frames[i][top:bottom,left:right,:],(112,112)))\n","            except:\n","              pass\n","          frames = []\n","    try:\n","      del top,right,bottom,left\n","    except:\n","      pass\n","    out.release()"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting face_recognition\n","  Downloading face_recognition-1.3.0-py2.py3-none-any.whl (15 kB)\n","Collecting face-recognition-models>=0.3.0 (from face_recognition)\n","  Downloading face_recognition_models-0.3.0.tar.gz (100.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.1/100.1 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.10/dist-packages (from face_recognition) (8.1.7)\n","Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.10/dist-packages (from face_recognition) (19.24.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from face_recognition) (1.23.5)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from face_recognition) (9.4.0)\n","Building wheels for collected packages: face-recognition-models\n","  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566170 sha256=55420ab1cd5b2d1bd563d165ff4b2d7c1724af6f24c14c978cc4ef1546d63e9f\n","  Stored in directory: /root/.cache/pip/wheels/7a/eb/cf/e9eced74122b679557f597bb7c8e4c739cfcac526db1fd523d\n","Successfully built face-recognition-models\n","Installing collected packages: face-recognition-models, face_recognition\n","Successfully installed face-recognition-models-0.3.0 face_recognition-1.3.0\n"]}]},{"cell_type":"code","metadata":{"id":"sF5qiWGLvei-","colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["2fdf56fc1aa346bd902c332ccf4166be","3e90e01f2406459c9ef8da084b397fe6","a537cabfd10647e785b124b9d1c3fa53","136612d43211445b86df53d153d77cf2","41fe1cba59ef446cac056e9240a0b6b3","af52a9b8477e40a4933abf1b7f6878aa","e9e5f01a478d40f78d6f77e81a6dd97f","980678cc21a64ce4baf385df8089ab0d","14f2904e6ece4edc88b6137c6a23873c","62f1097a2a7d4f10a0593105d6aee620","c4c053125af8476192e70f45169caa39"]},"executionInfo":{"status":"ok","timestamp":1703672315411,"user_tz":-330,"elapsed":523741,"user":{"displayName":"Jahan Sai","userId":"17887681191178947053"}},"outputId":"4c88fb4e-1bb3-4c5c-c235-74333973ce3c"},"source":["create_face_videos(video_files,'/content/drive/My Drive/Face_data/')"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["No of videos already present  0\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/47 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fdf56fc1aa346bd902c332ccf4166be"}},"metadata":{}}]}]}